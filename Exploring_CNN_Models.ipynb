{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtVl6gfgxZ7BhuQaMK3TMR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keelinarseneault/ML-Engineering/blob/main/Exploring_CNN_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Dd2sY5gyEpNe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_2w3cdnhw2w",
        "outputId": "62ffa686-53f2-4ac5-98b5-8f3ca8dbae82"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/Users/karseneault/Desktop/train_data/'\n",
        "test_path = '/Users/karseneault/Desktop/test_data_v2/'"
      ],
      "metadata": {
        "id": "gC8tnQfvEyk_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('drive/MyDrive/train.csv')\n",
        "test = pd.read_csv('drive/MyDrive/test.csv')"
      ],
      "metadata": {
        "id": "cMJcSaZCE2Yd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[['file_name', 'label']]\n",
        "train.columns = ['id', 'label']"
      ],
      "metadata": {
        "id": "68Xug_ZyE3JG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeDeW4SHFI6O",
        "outputId": "8153f36d-df06-47c4-c41e-2d946cddf2bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(79950, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.value_counts('label'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AojQFDtlOnoN",
        "outputId": "96c2c6f3-409f-4cc8-f0ce-3a1d4434a06b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "0    39975\n",
            "1    39975\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compare CNN Architectures**"
      ],
      "metadata": {
        "id": "noOtFFLeoX1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConvNeXT:**"
      ],
      "metadata": {
        "id": "vEJ-iAd0onmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use PyTorch\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "kUm3PDpVo-UP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'drive/MyDrive/Images'"
      ],
      "metadata": {
        "id": "bOeGtoigqJ32"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_exists(id):\n",
        "    filepath = f\"drive/MyDrive/Images/{id}\"\n",
        "    return os.path.isfile(filepath)"
      ],
      "metadata": {
        "id": "-xlBbZ9BpC5N"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[train[\"id\"].apply(image_exists)]"
      ],
      "metadata": {
        "id": "dA3xreAJpG65"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHHe14XEpNzl",
        "outputId": "bfb8ce77-769a-4a37-b5c6-7a1e57bcfc32"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11040, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Take a random sample of the training set in order to train on a smaller set of images, while maintaining the balanced ratio between the two classes:**"
      ],
      "metadata": {
        "id": "zGMI4kxaqvYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample = train.groupby(\"label\", group_keys=False).apply(lambda x:x.sample(frac=0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R09SM0sypXHl",
        "outputId": "8b178933-42ef-4592-e141-8c55f37edb6a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-137aab85c240>:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  train_sample = train.groupby(\"label\", group_keys=False).apply(lambda x:x.sample(frac=0.5))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(\n",
        "    train_sample,\n",
        "    test_size=0.05,\n",
        "    random_state=42,\n",
        "    stratify=train_sample['label']\n",
        ")"
      ],
      "metadata": {
        "id": "mBVV8KVRp_nK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shapes of the splits\n",
        "print(f'Train shape: {train_df.shape}')\n",
        "print(f'Validation shape: {val_df.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6H40J4asrXp",
        "outputId": "00c028f6-c637-4ad7-b49b-d3731d08b01c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (5244, 2)\n",
            "Validation shape: (276, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AIImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, root_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.dataframe.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.dataframe.iloc[idx, 1]\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "KkRpzFyZpjAS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize(232),  # Resize to match ConvNeXt preprocessing\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "_qx3UQXmpfGt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = AIImageDataset(train_df, root_dir=path, transform=train_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "StE9uFGTplL0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained ConvNeXt Base model\n",
        "model = models.convnext_base(weights=\"DEFAULT\")\n",
        "\n",
        "# Freeze all layers initially\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the last two stages\n",
        "for param in model.features[-2:].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Replace the classifier head with a custom one\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling\n",
        "    nn.Flatten(),                  # Flatten the tensor\n",
        "    nn.BatchNorm1d(1024),          # Add BatchNorm here\n",
        "    nn.Linear(1024, 512),          # First fully connected layer\n",
        "    nn.ReLU(),                     # Activation function\n",
        "    nn.Dropout(0.4),               # Dropout for regularization\n",
        "    nn.Linear(512, 2)              # Output layer (binary classification)\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.features[-2:].parameters(), 'lr': 1e-5},  # Lower LR for backbone\n",
        "    {'params': model.classifier.parameters(), 'lr': 1e-4}      # Higher LR for classifier\n",
        "])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.7)"
      ],
      "metadata": {
        "id": "WkVX01BSqYKO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irRvVe7U0mWP",
        "outputId": "4f6b2128-6250-4217-a5c0-f3662cda0aaf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "epochs = 5\n",
        "\n",
        "train_losses, train_accuracies, val_losses, val_accuracies, val_f1s = [], [], [], [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_accuracy = 0.0\n",
        "\n",
        "    for data, label in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        preds = output.argmax(dim=1)\n",
        "        acc = (preds == label).float().mean().item()\n",
        "        epoch_accuracy += acc\n",
        "\n",
        "    epoch_loss /= len(train_loader)\n",
        "    epoch_accuracy /= len(train_loader)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_accuracy)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{epochs}] \"\n",
        "        f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_accuracy:.4f} | \")\n",
        "\n",
        "  # Step the learning rate scheduler\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0s32hOcqbvN",
        "outputId": "44ac23f1-1585-4036-f0f3-173e24cf5be1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 164/164 [45:18<00:00, 16.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] Train Loss: 0.4323 | Train Acc: 0.8201 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 164/164 [44:24<00:00, 16.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/5] Train Loss: 0.2628 | Train Acc: 0.8989 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 164/164 [41:49<00:00, 15.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/5] Train Loss: 0.2163 | Train Acc: 0.9163 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 164/164 [45:51<00:00, 16.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/5] Train Loss: 0.1891 | Train Acc: 0.9246 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 164/164 [46:10<00:00, 16.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/5] Train Loss: 0.1868 | Train Acc: 0.9210 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **InceptionV3:**"
      ],
      "metadata": {
        "id": "_OIabZVOqfoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TensorFlow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "RoDiZymWrcqS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'drive/MyDrive/Images/'"
      ],
      "metadata": {
        "id": "ciUpAWRrEfe6"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess an image\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "  img = load_img(image_path, target_size=(299, 299))  # Adjust target_size if needed\n",
        "  img = img_to_array(img)\n",
        "  img = img / 255.0  # Normalize pixel values\n",
        "  img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "  return img\n",
        "\n",
        "# Preprocess training data\n",
        "train_images = []\n",
        "train_labels = []\n",
        "for index, row in train_df.iterrows():\n",
        "    image_path = os.path.join(path, row['id'])\n",
        "    train_images.append(preprocess_image(image_path))\n",
        "    train_labels.append(row['label'])\n",
        "\n",
        "train_images = np.array(train_images)\n",
        "train_labels = np.array(train_labels)\n"
      ],
      "metadata": {
        "id": "U829xZ8xByTO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare validation data\n",
        "\n",
        "val_images = []\n",
        "val_labels = []\n",
        "for index, row in val_df.iterrows():\n",
        "    image_path = os.path.join(path, row['id'])\n",
        "    val_images.append(preprocess_image(image_path))\n",
        "    val_labels.append(row['label'])\n",
        "\n",
        "val_images = np.concatenate(val_images, axis=0)\n",
        "val_labels = np.array(val_labels)"
      ],
      "metadata": {
        "id": "gUZM08sLDnbu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "base_model.trainable = False"
      ],
      "metadata": {
        "id": "-FNB0cAXsCU3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add layers\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)  # Prevent overfitting\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
        "\n",
        "# Create final model\n",
        "model = Model(inputs=base_model.input, outputs=output)"
      ],
      "metadata": {
        "id": "K6ygk8O3sOnR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam',\n",
        "                    loss='binary_crossentropy',\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "3PM_pIcHsWtc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_history = model.fit(train_images, train_labels, epochs=5, validation_data=(val_images, val_labels))"
      ],
      "metadata": {
        "id": "zrLz_XIOsZrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the training data\n",
        "train_loss, train_accuracy = model.evaluate(train_dataset)\n",
        "\n",
        "print(f\"Accuracy on train data: {train_accuracy:.2%} | Loss: {train_loss:.4f}\")"
      ],
      "metadata": {
        "id": "NpjPbv02s8rc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}